{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjay55/CS284_PA3/blob/main/train_valid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dTLm1pMWboAW"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9bamMuGFboAb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pickle\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "\n",
        "# from packaging import version\n",
        "import pandas as pd\n",
        "import tensorboard as tb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wWISoVx79HBJ"
      },
      "outputs": [],
      "source": [
        "def backup(object, filename):\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(object, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def reload(persist):\n",
        "    with open(persist, 'rb') as f:\n",
        "        # The protocol version used is detected automatically, so we do not\n",
        "        # have to specify it.\n",
        "        data = pickle.load(f)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oVMPTa9tkHWT"
      },
      "outputs": [],
      "source": [
        "path = \".\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PMDsMs6wi2Qa"
      },
      "outputs": [],
      "source": [
        "class PerImageStandardization(object):\n",
        "      \"\"\"\n",
        "      per_image_whitening function equivalent in PyTorch\n",
        "      \"\"\"\n",
        "      def __call__(self, img):\n",
        "\n",
        "        adjusted_stddev = torch.max(torch.std(img), 1/torch.sqrt(torch.tensor(torch.numel(img))))\n",
        "\n",
        "        return (img-torch.mean(img))/adjusted_stddev\n",
        "        \n",
        "      def __repr__(self):\n",
        "        return self.__class__.__name__+'()' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOHpFFdGj0xH",
        "outputId": "a90bfe13-0480-45bb-b0a8-b9382ef0686a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "#Sequence of steps in image transformation\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.CenterCrop(size=28),\n",
        "     PerImageStandardization()])\n",
        "\n",
        "#train dataset is obtained\n",
        "trainset = torchvision.datasets.CIFAR10(root=path, train=True,\n",
        "                                        download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEU1bb6DboAc",
        "outputId": "28940480-d52b-46a9-fa9b-e39a3f5e6d2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#test dataset is obtained\n",
        "batch_size = 16\n",
        "\n",
        "#object for distributing data during training and test is defined\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root=path, train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=0)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WvA4NN3WboAf"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding=0):\n",
        "        super().__init__()\n",
        "\n",
        "        #accessible attributes\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, padding_mode='zeros')\n",
        "        self.batch = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = F.relu(self.batch(x))\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definition of Neural Network Modules and overall network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_K7XfNZnJMgL"
      },
      "outputs": [],
      "source": [
        "class Inception(nn.Module):\n",
        "    def __init__(self, in_channels, ch1, ch3):\n",
        "        super().__init__()\n",
        "\n",
        "        #accessible attributes\n",
        "        self.ch1 = ch1\n",
        "        self.ch3 = ch3\n",
        "\n",
        "        self.convch1 = Conv(in_channels, ch1, 1, 1)\n",
        "        self.convch3 = Conv(in_channels, ch3, 3, 1, 1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        ch1features = self.convch1(x)\n",
        "        ch3features = self.convch3(x)\n",
        "        x = torch.cat((ch1features,ch3features),1)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "J1hFJhywLoew"
      },
      "outputs": [],
      "source": [
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_channels, ch3):\n",
        "        super().__init__()\n",
        "\n",
        "        #accessible attributes\n",
        "        self.ch3 = ch3\n",
        "\n",
        "        self.convch3 = Conv(in_channels, ch3, 3, 2)\n",
        "        self.maxpool = nn.MaxPool2d(3, 2)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        ch3features = self.convch3(x)\n",
        "        x_maxpool = self.maxpool(x)\n",
        "        x = torch.cat((ch3features,x_maxpool),1)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dH0Rp4JRf6kY"
      },
      "outputs": [],
      "source": [
        "class MiniGoogleNet(nn.Module):\n",
        "    def __init__(self, in_channels, dropoutval):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.dropoutval = dropoutval\n",
        "\n",
        "        self.conv_in = Conv(in_channels, 96, 3, 1)\n",
        "\n",
        "        self.inception1a = Inception(96, 32, 32)\n",
        "        self.inception1b = Inception(64, 32, 48)\n",
        "        self.downsample1 = Downsample(32+48, 80)\n",
        "\n",
        "        self.inception2a = Inception(80+32+48, 112, 48)\n",
        "        self.inception2b = Inception(112+48, 96, 64)\n",
        "        self.inception2c = Inception(96+64, 80, 80)\n",
        "        self.inception2d = Inception(160, 48, 96)\n",
        "        self.downsample2 = Downsample(48+96, 96)\n",
        "\n",
        "        self.inception3a = Inception(48+96+96, 176, 160)\n",
        "        self.inception3b = Inception(176+160, 176, 160)       \n",
        "        self.meanpool = nn.AdaptiveAvgPool2d(1) #1 is for global mean pooling\n",
        "        self.dropout = nn.Dropout(self.dropoutval)\n",
        "        self.fcout = nn.Linear(176+160, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_in(x)\n",
        "\n",
        "        x = self.inception1a(x)\n",
        "        x = self.inception1b(x)\n",
        "        x = self.downsample1(x)\n",
        "\n",
        "        x = self.inception2a(x)\n",
        "        x = self.inception2b(x)\n",
        "        x = self.inception2c(x)\n",
        "        x = self.inception2d(x)\n",
        "        x = self.downsample2(x)\n",
        "\n",
        "        x = self.inception3a(x)\n",
        "        x = self.inception3b(x)\n",
        "        x = self.meanpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fcout(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "p7JYZ9KuGLBg"
      },
      "outputs": [],
      "source": [
        "net = MiniGoogleNet(3, 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfNRp2rzboAg",
        "outputId": "99b602e3-73ae-4503-aa5a-f9b71575f542"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adjusting learning rate of group 0 to 1.0000e-01.\n"
          ]
        }
      ],
      "source": [
        "#optimization criteria\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)\n",
        "scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=1, end_factor=0.01, total_iters=30, last_epoch=-1, verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8_eIPZRF3bFR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MiniGoogleNet(\n",
              "  (conv_in): Conv(\n",
              "    (conv): Conv2d(3, 96, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (batch): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (inception1a): Inception(\n",
              "    (convch1): Conv(\n",
              "      (conv): Conv2d(96, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (batch): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (convch3): Conv(\n",
              "      (conv): Conv2d(96, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (inception1b): Inception(\n",
              "    (convch1): Conv(\n",
              "      (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (batch): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (convch3): Conv(\n",
              "      (conv): Conv2d(64, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (downsample1): Downsample(\n",
              "    (convch3): Conv(\n",
              "      (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(2, 2))\n",
              "      (batch): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (inception2a): Inception(\n",
              "    (convch1): Conv(\n",
              "      (conv): Conv2d(160, 112, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (batch): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (convch3): Conv(\n",
              "      (conv): Conv2d(160, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (inception2b): Inception(\n",
              "    (convch1): Conv(\n",
              "      (conv): Conv2d(160, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (batch): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (convch3): Conv(\n",
              "      (conv): Conv2d(160, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (inception2c): Inception(\n",
              "    (convch1): Conv(\n",
              "      (conv): Conv2d(160, 80, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (batch): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (convch3): Conv(\n",
              "      (conv): Conv2d(160, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (inception2d): Inception(\n",
              "    (convch1): Conv(\n",
              "      (conv): Conv2d(160, 48, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (batch): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (convch3): Conv(\n",
              "      (conv): Conv2d(160, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (downsample2): Downsample(\n",
              "    (convch3): Conv(\n",
              "      (conv): Conv2d(144, 96, kernel_size=(3, 3), stride=(2, 2))\n",
              "      (batch): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (inception3a): Inception(\n",
              "    (convch1): Conv(\n",
              "      (conv): Conv2d(240, 176, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (batch): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (convch3): Conv(\n",
              "      (conv): Conv2d(240, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (inception3b): Inception(\n",
              "    (convch1): Conv(\n",
              "      (conv): Conv2d(336, 176, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (batch): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (convch3): Conv(\n",
              "      (conv): Conv2d(336, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (batch): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (meanpool): AdaptiveAvgPool2d(output_size=1)\n",
              "  (dropout): Dropout(p=0.9, inplace=False)\n",
              "  (fcout): Linear(in_features=336, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "\n",
        "print(device)\n",
        "\n",
        "net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GAL14rKS7hxp"
      },
      "outputs": [],
      "source": [
        "#tensorboard setup\n",
        "run_suffix = \"Dropout_0p1\"\n",
        "writer = SummaryWriter(comment=\"_{}\".format(run_suffix))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "HRwzet2zboAh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   600] loss: 2.949\n",
            "[1,  1200] loss: 2.316\n",
            "[1,  1800] loss: 2.315\n",
            "[1,  2400] loss: 2.315\n",
            "[1,  3000] loss: 2.317\n",
            "Adjusting learning rate of group 0 to 9.6700e-02.\n",
            "Accuracy of the network on the 10000 test images: 10 %\n",
            "[2,   600] loss: 2.315\n",
            "[2,  1200] loss: 2.315\n",
            "[2,  1800] loss: 2.315\n",
            "[2,  2400] loss: 2.313\n",
            "[2,  3000] loss: 2.316\n",
            "Adjusting learning rate of group 0 to 9.3400e-02.\n",
            "Accuracy of the network on the 10000 test images: 10 %\n",
            "[3,   600] loss: 2.314\n",
            "[3,  1200] loss: 2.313\n",
            "[3,  1800] loss: 2.314\n",
            "[3,  2400] loss: 2.312\n",
            "[3,  3000] loss: 2.313\n",
            "Adjusting learning rate of group 0 to 9.0100e-02.\n",
            "Accuracy of the network on the 10000 test images: 10 %\n",
            "[4,   600] loss: 2.313\n",
            "[4,  1200] loss: 2.313\n",
            "[4,  1800] loss: 2.313\n",
            "[4,  2400] loss: 2.312\n",
            "[4,  3000] loss: 2.316\n",
            "Adjusting learning rate of group 0 to 8.6800e-02.\n",
            "Accuracy of the network on the 10000 test images: 10 %\n",
            "[5,   600] loss: 2.312\n",
            "[5,  1200] loss: 2.314\n",
            "[5,  1800] loss: 2.314\n",
            "[5,  2400] loss: 2.312\n",
            "[5,  3000] loss: 2.314\n",
            "Adjusting learning rate of group 0 to 8.3500e-02.\n",
            "Accuracy of the network on the 10000 test images: 10 %\n",
            "[6,   600] loss: 2.313\n",
            "[6,  1200] loss: 2.313\n",
            "[6,  1800] loss: 2.312\n",
            "[6,  2400] loss: 2.313\n",
            "[6,  3000] loss: 2.313\n",
            "Adjusting learning rate of group 0 to 8.0200e-02.\n",
            "Accuracy of the network on the 10000 test images: 10 %\n",
            "[7,   600] loss: 2.310\n",
            "[7,  1200] loss: 2.313\n",
            "[7,  1800] loss: 2.309\n",
            "[7,  2400] loss: 2.307\n",
            "[7,  3000] loss: 2.310\n",
            "Adjusting learning rate of group 0 to 7.6900e-02.\n",
            "Accuracy of the network on the 10000 test images: 10 %\n",
            "[8,   600] loss: 2.309\n",
            "[8,  1200] loss: 2.311\n",
            "[8,  1800] loss: 2.312\n",
            "[8,  2400] loss: 2.310\n",
            "[8,  3000] loss: 2.309\n",
            "Adjusting learning rate of group 0 to 7.3600e-02.\n",
            "Accuracy of the network on the 10000 test images: 10 %\n",
            "[9,   600] loss: 2.309\n",
            "[9,  1200] loss: 2.309\n",
            "[9,  1800] loss: 2.306\n",
            "[9,  2400] loss: 2.307\n",
            "[9,  3000] loss: 2.308\n",
            "Adjusting learning rate of group 0 to 7.0300e-02.\n",
            "Accuracy of the network on the 10000 test images: 10 %\n",
            "[10,   600] loss: 2.306\n",
            "[10,  1200] loss: 2.307\n",
            "[10,  1800] loss: 2.307\n",
            "[10,  2400] loss: 2.301\n",
            "[10,  3000] loss: 2.303\n",
            "Adjusting learning rate of group 0 to 6.7000e-02.\n",
            "Accuracy of the network on the 10000 test images: 10 %\n",
            "[11,   600] loss: 2.305\n",
            "[11,  1200] loss: 2.300\n",
            "[11,  1800] loss: 2.304\n",
            "[11,  2400] loss: 2.299\n",
            "[11,  3000] loss: 2.301\n",
            "Adjusting learning rate of group 0 to 6.3700e-02.\n",
            "Accuracy of the network on the 10000 test images: 10 %\n",
            "[12,   600] loss: 2.300\n",
            "[12,  1200] loss: 2.292\n",
            "[12,  1800] loss: 2.290\n",
            "[12,  2400] loss: 2.285\n",
            "[12,  3000] loss: 2.283\n",
            "Adjusting learning rate of group 0 to 6.0400e-02.\n",
            "Accuracy of the network on the 10000 test images: 12 %\n",
            "[13,   600] loss: 2.274\n",
            "[13,  1200] loss: 2.274\n",
            "[13,  1800] loss: 2.267\n",
            "[13,  2400] loss: 2.252\n",
            "[13,  3000] loss: 2.233\n",
            "Adjusting learning rate of group 0 to 5.7100e-02.\n",
            "Accuracy of the network on the 10000 test images: 13 %\n",
            "[14,   600] loss: 2.228\n",
            "[14,  1200] loss: 2.230\n",
            "[14,  1800] loss: 2.226\n",
            "[14,  2400] loss: 2.213\n",
            "[14,  3000] loss: 2.201\n",
            "Adjusting learning rate of group 0 to 5.3800e-02.\n",
            "Accuracy of the network on the 10000 test images: 15 %\n",
            "[15,   600] loss: 2.197\n",
            "[15,  1200] loss: 2.172\n",
            "[15,  1800] loss: 2.167\n",
            "[15,  2400] loss: 2.170\n",
            "[15,  3000] loss: 2.154\n",
            "Adjusting learning rate of group 0 to 5.0500e-02.\n",
            "Accuracy of the network on the 10000 test images: 15 %\n",
            "[16,   600] loss: 2.153\n",
            "[16,  1200] loss: 2.142\n",
            "[16,  1800] loss: 2.147\n",
            "[16,  2400] loss: 2.149\n",
            "[16,  3000] loss: 2.137\n",
            "Adjusting learning rate of group 0 to 4.7200e-02.\n",
            "Accuracy of the network on the 10000 test images: 15 %\n",
            "[17,   600] loss: 2.137\n",
            "[17,  1200] loss: 2.135\n",
            "[17,  1800] loss: 2.121\n",
            "[17,  2400] loss: 2.128\n",
            "[17,  3000] loss: 2.124\n",
            "Adjusting learning rate of group 0 to 4.3900e-02.\n",
            "Accuracy of the network on the 10000 test images: 15 %\n",
            "[18,   600] loss: 2.109\n",
            "[18,  1200] loss: 2.121\n",
            "[18,  1800] loss: 2.119\n",
            "[18,  2400] loss: 2.116\n",
            "[18,  3000] loss: 2.109\n",
            "Adjusting learning rate of group 0 to 4.0600e-02.\n",
            "Accuracy of the network on the 10000 test images: 16 %\n",
            "[19,   600] loss: 2.105\n",
            "[19,  1200] loss: 2.114\n",
            "[19,  1800] loss: 2.100\n",
            "[19,  2400] loss: 2.093\n",
            "[19,  3000] loss: 2.103\n",
            "Adjusting learning rate of group 0 to 3.7300e-02.\n",
            "Accuracy of the network on the 10000 test images: 15 %\n",
            "[20,   600] loss: 2.099\n",
            "[20,  1200] loss: 2.090\n",
            "[20,  1800] loss: 2.110\n",
            "[20,  2400] loss: 2.093\n",
            "[20,  3000] loss: 2.079\n",
            "Adjusting learning rate of group 0 to 3.4000e-02.\n",
            "Accuracy of the network on the 10000 test images: 15 %\n",
            "[21,   600] loss: 2.083\n",
            "[21,  1200] loss: 2.091\n",
            "[21,  1800] loss: 2.090\n",
            "[21,  2400] loss: 2.087\n",
            "[21,  3000] loss: 2.086\n",
            "Adjusting learning rate of group 0 to 3.0700e-02.\n",
            "Accuracy of the network on the 10000 test images: 16 %\n",
            "[22,   600] loss: 2.092\n",
            "[22,  1200] loss: 2.078\n",
            "[22,  1800] loss: 2.083\n",
            "[22,  2400] loss: 2.077\n",
            "[22,  3000] loss: 2.080\n",
            "Adjusting learning rate of group 0 to 2.7400e-02.\n",
            "Accuracy of the network on the 10000 test images: 16 %\n",
            "[23,   600] loss: 2.084\n",
            "[23,  1200] loss: 2.063\n",
            "[23,  1800] loss: 2.070\n",
            "[23,  2400] loss: 2.070\n",
            "[23,  3000] loss: 2.069\n",
            "Adjusting learning rate of group 0 to 2.4100e-02.\n",
            "Accuracy of the network on the 10000 test images: 16 %\n",
            "[24,   600] loss: 2.077\n",
            "[24,  1200] loss: 2.069\n",
            "[24,  1800] loss: 2.067\n",
            "[24,  2400] loss: 2.055\n",
            "[24,  3000] loss: 2.065\n",
            "Adjusting learning rate of group 0 to 2.0800e-02.\n",
            "Accuracy of the network on the 10000 test images: 15 %\n",
            "[25,   600] loss: 2.061\n",
            "[25,  1200] loss: 2.053\n",
            "[25,  1800] loss: 2.065\n",
            "[25,  2400] loss: 2.054\n",
            "[25,  3000] loss: 2.054\n",
            "Adjusting learning rate of group 0 to 1.7500e-02.\n",
            "Accuracy of the network on the 10000 test images: 16 %\n",
            "[26,   600] loss: 2.043\n",
            "[26,  1200] loss: 2.047\n",
            "[26,  1800] loss: 2.035\n",
            "[26,  2400] loss: 2.053\n",
            "[26,  3000] loss: 2.036\n",
            "Adjusting learning rate of group 0 to 1.4200e-02.\n",
            "Accuracy of the network on the 10000 test images: 17 %\n",
            "[27,   600] loss: 2.036\n",
            "[27,  1200] loss: 2.042\n",
            "[27,  1800] loss: 2.044\n",
            "[27,  2400] loss: 2.046\n",
            "[27,  3000] loss: 2.041\n",
            "Adjusting learning rate of group 0 to 1.0900e-02.\n",
            "Accuracy of the network on the 10000 test images: 17 %\n",
            "[28,   600] loss: 2.027\n",
            "[28,  1200] loss: 2.034\n",
            "[28,  1800] loss: 2.033\n",
            "[28,  2400] loss: 2.032\n",
            "[28,  3000] loss: 2.031\n",
            "Adjusting learning rate of group 0 to 7.6000e-03.\n",
            "Accuracy of the network on the 10000 test images: 18 %\n",
            "[29,   600] loss: 2.025\n",
            "[29,  1200] loss: 2.032\n",
            "[29,  1800] loss: 2.028\n",
            "[29,  2400] loss: 2.025\n",
            "[29,  3000] loss: 2.027\n",
            "Adjusting learning rate of group 0 to 4.3000e-03.\n",
            "Accuracy of the network on the 10000 test images: 18 %\n",
            "[30,   600] loss: 2.013\n",
            "[30,  1200] loss: 2.020\n",
            "[30,  1800] loss: 2.000\n",
            "[30,  2400] loss: 2.012\n",
            "[30,  3000] loss: 2.011\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 19 %\n",
            "[31,   600] loss: 2.017\n",
            "[31,  1200] loss: 2.009\n",
            "[31,  1800] loss: 2.011\n",
            "[31,  2400] loss: 2.002\n",
            "[31,  3000] loss: 2.014\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 19 %\n",
            "[32,   600] loss: 2.006\n",
            "[32,  1200] loss: 2.019\n",
            "[32,  1800] loss: 2.008\n",
            "[32,  2400] loss: 2.005\n",
            "[32,  3000] loss: 2.023\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 19 %\n",
            "[33,   600] loss: 2.006\n",
            "[33,  1200] loss: 2.013\n",
            "[33,  1800] loss: 2.007\n",
            "[33,  2400] loss: 2.010\n",
            "[33,  3000] loss: 1.999\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 19 %\n",
            "[34,   600] loss: 2.014\n",
            "[34,  1200] loss: 2.015\n",
            "[34,  1800] loss: 2.006\n",
            "[34,  2400] loss: 2.008\n",
            "[34,  3000] loss: 2.005\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 19 %\n",
            "[35,   600] loss: 2.000\n",
            "[35,  1200] loss: 2.002\n",
            "[35,  1800] loss: 2.002\n",
            "[35,  2400] loss: 2.008\n",
            "[35,  3000] loss: 2.016\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 19 %\n",
            "[36,   600] loss: 2.010\n",
            "[36,  1200] loss: 2.010\n",
            "[36,  1800] loss: 2.010\n",
            "[36,  2400] loss: 2.008\n",
            "[36,  3000] loss: 1.993\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 19 %\n",
            "[37,   600] loss: 1.999\n",
            "[37,  1200] loss: 2.006\n",
            "[37,  1800] loss: 2.003\n",
            "[37,  2400] loss: 2.015\n",
            "[37,  3000] loss: 2.000\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 19 %\n",
            "[38,   600] loss: 2.005\n",
            "[38,  1200] loss: 1.995\n",
            "[38,  1800] loss: 2.007\n",
            "[38,  2400] loss: 2.005\n",
            "[38,  3000] loss: 2.001\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 19 %\n",
            "[39,   600] loss: 1.996\n",
            "[39,  1200] loss: 2.004\n",
            "[39,  1800] loss: 1.989\n",
            "[39,  2400] loss: 2.004\n",
            "[39,  3000] loss: 2.004\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 19 %\n",
            "[40,   600] loss: 1.991\n",
            "[40,  1200] loss: 2.005\n",
            "[40,  1800] loss: 1.989\n",
            "[40,  2400] loss: 1.996\n",
            "[40,  3000] loss: 2.004\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[41,   600] loss: 1.996\n",
            "[41,  1200] loss: 2.013\n",
            "[41,  1800] loss: 1.991\n",
            "[41,  2400] loss: 1.994\n",
            "[41,  3000] loss: 2.010\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[42,   600] loss: 1.996\n",
            "[42,  1200] loss: 1.992\n",
            "[42,  1800] loss: 1.999\n",
            "[42,  2400] loss: 1.993\n",
            "[42,  3000] loss: 1.982\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[43,   600] loss: 1.994\n",
            "[43,  1200] loss: 1.978\n",
            "[43,  1800] loss: 1.978\n",
            "[43,  2400] loss: 1.990\n",
            "[43,  3000] loss: 1.979\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[44,   600] loss: 1.977\n",
            "[44,  1200] loss: 1.978\n",
            "[44,  1800] loss: 1.980\n",
            "[44,  2400] loss: 1.972\n",
            "[44,  3000] loss: 1.981\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[45,   600] loss: 1.976\n",
            "[45,  1200] loss: 1.987\n",
            "[45,  1800] loss: 1.974\n",
            "[45,  2400] loss: 1.971\n",
            "[45,  3000] loss: 1.977\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[46,   600] loss: 1.989\n",
            "[46,  1200] loss: 1.976\n",
            "[46,  1800] loss: 1.981\n",
            "[46,  2400] loss: 1.973\n",
            "[46,  3000] loss: 1.967\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[47,   600] loss: 1.973\n",
            "[47,  1200] loss: 1.979\n",
            "[47,  1800] loss: 1.988\n",
            "[47,  2400] loss: 1.966\n",
            "[47,  3000] loss: 1.982\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[48,   600] loss: 1.983\n",
            "[48,  1200] loss: 1.963\n",
            "[48,  1800] loss: 1.971\n",
            "[48,  2400] loss: 1.972\n",
            "[48,  3000] loss: 1.982\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 19 %\n",
            "[49,   600] loss: 1.971\n",
            "[49,  1200] loss: 1.964\n",
            "[49,  1800] loss: 1.972\n",
            "[49,  2400] loss: 1.965\n",
            "[49,  3000] loss: 1.974\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[50,   600] loss: 1.972\n",
            "[50,  1200] loss: 1.960\n",
            "[50,  1800] loss: 1.979\n",
            "[50,  2400] loss: 1.974\n",
            "[50,  3000] loss: 1.968\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[51,   600] loss: 1.969\n",
            "[51,  1200] loss: 1.963\n",
            "[51,  1800] loss: 1.968\n",
            "[51,  2400] loss: 1.969\n",
            "[51,  3000] loss: 1.963\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[52,   600] loss: 1.968\n",
            "[52,  1200] loss: 1.971\n",
            "[52,  1800] loss: 1.961\n",
            "[52,  2400] loss: 1.968\n",
            "[52,  3000] loss: 1.958\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[53,   600] loss: 1.968\n",
            "[53,  1200] loss: 1.961\n",
            "[53,  1800] loss: 1.966\n",
            "[53,  2400] loss: 1.970\n",
            "[53,  3000] loss: 1.980\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[54,   600] loss: 1.964\n",
            "[54,  1200] loss: 1.977\n",
            "[54,  1800] loss: 1.970\n",
            "[54,  2400] loss: 1.962\n",
            "[54,  3000] loss: 1.968\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[55,   600] loss: 1.975\n",
            "[55,  1200] loss: 1.959\n",
            "[55,  1800] loss: 1.969\n",
            "[55,  2400] loss: 1.958\n",
            "[55,  3000] loss: 1.947\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[56,   600] loss: 1.950\n",
            "[56,  1200] loss: 1.962\n",
            "[56,  1800] loss: 1.969\n",
            "[56,  2400] loss: 1.968\n",
            "[56,  3000] loss: 1.963\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[57,   600] loss: 1.960\n",
            "[57,  1200] loss: 1.960\n",
            "[57,  1800] loss: 1.967\n",
            "[57,  2400] loss: 1.970\n",
            "[57,  3000] loss: 1.960\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[58,   600] loss: 1.959\n",
            "[58,  1200] loss: 1.954\n",
            "[58,  1800] loss: 1.960\n",
            "[58,  2400] loss: 1.962\n",
            "[58,  3000] loss: 1.968\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[59,   600] loss: 1.967\n",
            "[59,  1200] loss: 1.970\n",
            "[59,  1800] loss: 1.960\n",
            "[59,  2400] loss: 1.964\n",
            "[59,  3000] loss: 1.959\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[60,   600] loss: 1.960\n",
            "[60,  1200] loss: 1.951\n",
            "[60,  1800] loss: 1.959\n",
            "[60,  2400] loss: 1.956\n",
            "[60,  3000] loss: 1.988\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[61,   600] loss: 1.957\n",
            "[61,  1200] loss: 1.954\n",
            "[61,  1800] loss: 1.963\n",
            "[61,  2400] loss: 1.954\n",
            "[61,  3000] loss: 1.958\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[62,   600] loss: 1.951\n",
            "[62,  1200] loss: 1.965\n",
            "[62,  1800] loss: 1.960\n",
            "[62,  2400] loss: 1.949\n",
            "[62,  3000] loss: 1.973\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[63,   600] loss: 1.952\n",
            "[63,  1200] loss: 1.965\n",
            "[63,  1800] loss: 1.952\n",
            "[63,  2400] loss: 1.948\n",
            "[63,  3000] loss: 1.962\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[64,   600] loss: 1.965\n",
            "[64,  1200] loss: 1.946\n",
            "[64,  1800] loss: 1.962\n",
            "[64,  2400] loss: 1.953\n",
            "[64,  3000] loss: 1.946\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 21 %\n",
            "[65,   600] loss: 1.950\n",
            "[65,  1200] loss: 1.948\n",
            "[65,  1800] loss: 1.946\n",
            "[65,  2400] loss: 1.943\n",
            "[65,  3000] loss: 1.944\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 20 %\n",
            "[66,   600] loss: 1.941\n",
            "[66,  1200] loss: 1.944\n",
            "[66,  1800] loss: 1.948\n",
            "[66,  2400] loss: 1.931\n",
            "[66,  3000] loss: 1.935\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 21 %\n",
            "[67,   600] loss: 1.940\n",
            "[67,  1200] loss: 1.939\n",
            "[67,  1800] loss: 1.939\n",
            "[67,  2400] loss: 1.929\n",
            "[67,  3000] loss: 1.939\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 21 %\n",
            "[68,   600] loss: 1.941\n",
            "[68,  1200] loss: 1.939\n",
            "[68,  1800] loss: 1.930\n",
            "[68,  2400] loss: 1.934\n",
            "[68,  3000] loss: 1.936\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 22 %\n",
            "[69,   600] loss: 1.934\n",
            "[69,  1200] loss: 1.930\n",
            "[69,  1800] loss: 1.930\n",
            "[69,  2400] loss: 1.920\n",
            "[69,  3000] loss: 1.932\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 21 %\n",
            "[70,   600] loss: 1.927\n",
            "[70,  1200] loss: 1.930\n",
            "[70,  1800] loss: 1.926\n",
            "[70,  2400] loss: 1.910\n",
            "[70,  3000] loss: 1.918\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 22 %\n",
            "[71,   600] loss: 1.912\n",
            "[71,  1200] loss: 1.923\n",
            "[71,  1800] loss: 1.912\n",
            "[71,  2400] loss: 1.916\n",
            "[71,  3000] loss: 1.919\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 21 %\n",
            "[72,   600] loss: 1.920\n",
            "[72,  1200] loss: 1.933\n",
            "[72,  1800] loss: 1.918\n",
            "[72,  2400] loss: 1.906\n",
            "[72,  3000] loss: 1.917\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 22 %\n",
            "[73,   600] loss: 1.914\n",
            "[73,  1200] loss: 1.927\n",
            "[73,  1800] loss: 1.912\n",
            "[73,  2400] loss: 1.913\n",
            "[73,  3000] loss: 1.915\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 21 %\n",
            "[74,   600] loss: 1.927\n",
            "[74,  1200] loss: 1.911\n",
            "[74,  1800] loss: 1.908\n",
            "[74,  2400] loss: 1.900\n",
            "[74,  3000] loss: 1.898\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 22 %\n",
            "[75,   600] loss: 1.911\n",
            "[75,  1200] loss: 1.895\n",
            "[75,  1800] loss: 1.903\n",
            "[75,  2400] loss: 1.899\n",
            "[75,  3000] loss: 1.900\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 22 %\n",
            "[76,   600] loss: 1.914\n",
            "[76,  1200] loss: 1.905\n",
            "[76,  1800] loss: 1.898\n",
            "[76,  2400] loss: 1.900\n",
            "[76,  3000] loss: 1.896\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 23 %\n",
            "[77,   600] loss: 1.904\n",
            "[77,  1200] loss: 1.893\n",
            "[77,  1800] loss: 1.904\n",
            "[77,  2400] loss: 1.896\n",
            "[77,  3000] loss: 1.895\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 23 %\n",
            "[78,   600] loss: 1.911\n",
            "[78,  1200] loss: 1.891\n",
            "[78,  1800] loss: 1.902\n",
            "[78,  2400] loss: 1.895\n",
            "[78,  3000] loss: 1.887\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 22 %\n",
            "[79,   600] loss: 1.896\n",
            "[79,  1200] loss: 1.893\n",
            "[79,  1800] loss: 1.887\n",
            "[79,  2400] loss: 1.886\n",
            "[79,  3000] loss: 1.886\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 22 %\n",
            "[80,   600] loss: 1.878\n",
            "[80,  1200] loss: 1.878\n",
            "[80,  1800] loss: 1.898\n",
            "[80,  2400] loss: 1.899\n",
            "[80,  3000] loss: 1.886\n",
            "Adjusting learning rate of group 0 to 1.0000e-03.\n",
            "Accuracy of the network on the 10000 test images: 22 %\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "#training proper\n",
        "interval = 600\n",
        "start_accuracy = 0\n",
        "validation_divisor = len(list(enumerate(testloader)))\n",
        "PATH = 'cifar_net_{}.pth'.format(run_suffix)\n",
        "\n",
        "counter_train = 0\n",
        "for epoch in range(80):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if i % interval == interval-1:  \n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / interval))\n",
        "            writer.add_scalar(\"Loss/train\", running_loss / interval, counter_train+1 , walltime = time.time())\n",
        "            counter_train+=1\n",
        "            running_loss = 0.0\n",
        "            \n",
        "            torch.save(net.state_dict(), 'cifar_net_{}_latest.pth'.format(run_suffix))\n",
        "\n",
        "    scheduler.step()\n",
        "    #do validation on every epoch\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    #validation\n",
        "    running_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            # images, labels = data\n",
        "            # calculate outputs by running images through the network \n",
        "            outputs = net(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            # the class with the highest energy is what we choose as prediction\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "        100 * correct / total))\n",
        "    writer.add_scalar(\"Accuracy/validation\", correct / total, epoch+1)\n",
        "    loss = criterion(outputs, labels)\n",
        "    validation_loss = running_loss/625\n",
        "    if correct / total >= start_accuracy:\n",
        "        torch.save(net.state_dict(), PATH)\n",
        "        start_accuracy = correct / total\n",
        "    writer.add_scalar(\"Loss/validation\", validation_loss, epoch+1)\n",
        "    \n",
        "writer.flush()\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boG1a_zQboAi"
      },
      "source": [
        "Next, let's load back in our saved model (note: saving and re-loading the model\n",
        "wasn't necessary here, we only did it to illustrate how to do so):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OtXO1YeboAj",
        "outputId": "c0e5f5e5-b72c-406d-c4ca-6668c2ff2321"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 27 %\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        # images, labels = data\n",
        "        # calculate outputs by running images through the network \n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouFRErHjboAk",
        "outputId": "61714fa8-93d6-4e87-aca7-7b26358d0c14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for class plane is: 43.2 %\n",
            "Accuracy for class car   is: 27.0 %\n",
            "Accuracy for class bird  is: 7.2 %\n",
            "Accuracy for class cat   is: 0.5 %\n",
            "Accuracy for class deer  is: 0.1 %\n",
            "Accuracy for class dog   is: 66.3 %\n",
            "Accuracy for class frog  is: 37.8 %\n",
            "Accuracy for class horse is: 11.8 %\n",
            "Accuracy for class ship  is: 43.2 %\n",
            "Accuracy for class truck is: 46.0 %\n"
          ]
        }
      ],
      "source": [
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        # images, labels = data    \n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = net(images)    \n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "  \n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname, \n",
        "                                                   accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks4BJfFXboAk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "train_valid.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
